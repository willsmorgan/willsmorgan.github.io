<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.41" />
  <meta name="author" content="Will Morgan">

  
  
  
  
    
      
    
  
  <meta name="description" content="MotivationOne of my colleagues has been working on a project analysing a particular type of student survey data. The data is partitioned into a handful of sub-surveys measuring (essentially) different categorizations of non-cognitive skills and personality/learning traits. One aim of the project was to evaluate how combinations of metrics from the individual tests could be used to predict student outcomes. There were over 100 variables he was intending to analyze, many of which had seemingly overlapping definitions, so collinearity was a big concern going into it.">

  
  <link rel="alternate" hreflang="en-us" href="/willsmorgan.github.io/post/solving-collinearity-with-penalized-regression/">

  


  

  
  
  <meta name="theme-color" content="#EF525B">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/willsmorgan.github.io/styles.css">
  

  

  
  <link rel="alternate" href="/willsmorgan.github.io/index.xml" type="application/rss+xml" title="William Morgan">
  <link rel="feed" href="/willsmorgan.github.io/index.xml" type="application/rss+xml" title="William Morgan">
  

  <link rel="manifest" href="/willsmorgan.github.io/site.webmanifest">
  <link rel="icon" type="image/png" href="/willsmorgan.github.io/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/willsmorgan.github.io/img/icon-192.png">

  <link rel="canonical" href="/willsmorgan.github.io/post/solving-collinearity-with-penalized-regression/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="William Morgan">
  <meta property="og:url" content="/willsmorgan.github.io/post/solving-collinearity-with-penalized-regression/">
  <meta property="og:title" content="Solving Collinearity with Penalized Regression | William Morgan">
  <meta property="og:description" content="MotivationOne of my colleagues has been working on a project analysing a particular type of student survey data. The data is partitioned into a handful of sub-surveys measuring (essentially) different categorizations of non-cognitive skills and personality/learning traits. One aim of the project was to evaluate how combinations of metrics from the individual tests could be used to predict student outcomes. There were over 100 variables he was intending to analyze, many of which had seemingly overlapping definitions, so collinearity was a big concern going into it.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-09-03T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2018-09-03T00:00:00&#43;00:00">
  

  
  

  <title>Solving Collinearity with Penalized Regression | William Morgan</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/willsmorgan.github.io/">William Morgan</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/willsmorgan.github.io/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/willsmorgan.github.io/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/willsmorgan.github.io/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/willsmorgan.github.io/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Solving Collinearity with Penalized Regression</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-09-03 00:00:00 &#43;0000 UTC" itemprop="datePublished dateModified">
      Sep 3, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Will Morgan">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/willsmorgan.github.io/categories/discussion/">Discussion</a
    >
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Solving%20Collinearity%20with%20Penalized%20Regression&amp;url=%2fwillsmorgan.github.io%2fpost%2fsolving-collinearity-with-penalized-regression%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fwillsmorgan.github.io%2fpost%2fsolving-collinearity-with-penalized-regression%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fwillsmorgan.github.io%2fpost%2fsolving-collinearity-with-penalized-regression%2f&amp;title=Solving%20Collinearity%20with%20Penalized%20Regression"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fwillsmorgan.github.io%2fpost%2fsolving-collinearity-with-penalized-regression%2f&amp;title=Solving%20Collinearity%20with%20Penalized%20Regression"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Solving%20Collinearity%20with%20Penalized%20Regression&amp;body=%2fwillsmorgan.github.io%2fpost%2fsolving-collinearity-with-penalized-regression%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      <div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>One of my colleagues has been working on a project analysing a particular type of student survey data. The data is partitioned into a handful of sub-surveys measuring (essentially) different categorizations of non-cognitive skills and personality/learning traits. One aim of the project was to evaluate how combinations of metrics from the individual tests could be used to predict student outcomes. There were over 100 variables he was intending to analyze, many of which had seemingly overlapping definitions, so collinearity was a big concern going into it. To get around the problem, this researcher chose to use penalized linear regression, claiming that it “solved” the problem of multicollinearity. I wasn’t familiar enough with the mechanics of the technique, so I decided to investigate more and find out what was going on underneath the hood.</p>
</div>
<div id="what-is-multicollinearity" class="section level2">
<h2>What is (multi)collinearity?</h2>
<p>In layman’s terms, collinearity occurs when some of the information being used to make predictions on a particular outcome is redundant. For example, say I tell you to predict a person’s wage given the number of years of education they have and their highest achieved degree. Although they aren’t exactly the same, you could almost perfectly predict their highest achieved degree by checking the number of years they’ve been in school. A person who’s completed 12 years of education is likely to only have their high school diploma, someone with 16 years will probably have their undergraduate degree, and so on. This situation doesn’t exemplify <strong>perfect</strong> collinearity, but the fact that these two bits of information are highly correlated can cause problems in linear regression. Likewise, multicollinearity can be thought of in the same way - redundant information. It is slightly more difficult to identify, but the idea is the same. Multicollinearity occurs when one variable can be explained by a combination of other variables.</p>
<p>Mathematically, perfect collinearity is defined as two variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> having an exactly linear relationship. That is, there exist two scalars <span class="math inline">\(a_0\)</span> and <span class="math inline">\(a_1\)</span>, at least one nonzero, such that:</p>
<p><span class="math display">\[X_2 = a_0 + a_1X_1\]</span></p>
<p>For reasons I’ll show later, we aren’t necessarily worried about <em>perfect</em> collinearity - the more common scenario is <em>near-perfect</em> collinearity. This definition is actually just an application of linear dependence, so the more general definition can just be rephrased as such - collinearity is defined as the existence of <span class="math inline">\(p\)</span> constants <span class="math inline">\(a_1,\dotsb, a_p\)</span>, not all zero, such that <span class="math inline">\(\sum_{i=1}^p a_iX_i = 0\)</span>. In matrix notation,</p>
<p><span class="math display">\[\boldsymbol{X}\boldsymbol{a} = \boldsymbol{0}\]</span>,</p>
<p>where <span class="math inline">\(\boldsymbol{a}\)</span> is a <span class="math inline">\(p \times 1\)</span> vector with at least one nonzero entry and <span class="math inline">\(\boldsymbol{X}\)</span> is the <span class="math inline">\(n \times p\)</span> model matrix.</p>
<p>As you can imagine, checking for collinearity can be as simple as looking for perfect correlation. Multicollinearity is not as simple, but many choose to look at the Variance Inflation Factor (VIF). I won’t go into further detail here, but VIF is essentially a measure of how much one variable is explained by the rest of the variables in your data.</p>
</div>
<div id="effects-of-multicollinearity" class="section level2">
<h2>Effects of Multicollinearity</h2>
<p>In the general sense, presence of multicollinearity can impact the validity of our parameter estimates. Theoretically it can keep us from finding a unique set of solutions, but more often the worry is about the variance (and standard error) of the estimates. In particular, recall the solution to the standard multiple regression problem:</p>
<p><span class="math display">\[\hat{\beta} = (\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\]</span></p>
<p>First of all, for there be a unique solution for <span class="math inline">\(\hat{\beta}\)</span> we need to guarantee that <span class="math inline">\((\boldsymbol{X}^{T}\boldsymbol{X})\)</span> is invertible. When <span class="math inline">\(X\)</span> has linearly dependent columns, then there exists a vector <span class="math inline">\(\boldsymbol{a}\)</span> (not all zero) such that:</p>
<p><span class="math display">\[\boldsymbol{X}\boldsymbol{a} = 0\]</span> If this is the case, then:</p>
<p><span class="math display">\[\|\boldsymbol{X}\boldsymbol{a}\|_2^{2} = \boldsymbol{a}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{a} = 0\]</span>,</p>
<p>and since <span class="math inline">\(\boldsymbol{a}\)</span> is a nonzero vector, <span class="math inline">\(\boldsymbol{X}^{T}\boldsymbol{X}\)</span> is not positive definite. Hence, <span class="math inline">\(\boldsymbol{X}^{T}\boldsymbol{X}\)</span> is not invertible.</p>
<p>In the case that our predictors are <strong>nearly</strong> collinear the matrix will still have an inverse, but the entries in that matrix will be massive. This is worrisome when we consider the equation for the variance of the estimates:</p>
<p><span class="math display">\[Var[\hat{\beta}] = \boldsymbol{\sigma^2}(\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\]</span></p>
<p>As this value increases the precision of our estimates worsen, making us far less certain about the range of possible values each estimate can take on. To illustrate, I’ll create a 3-column matrix, where the first two columns are random pulls from the standard normal distribution and the third column is a linear combination of the first two.</p>
<pre class="r"><code>set.seed(18)

### Create (almost) collinear matrix
x1 &lt;- rnorm(10000)
x2 &lt;- rnorm(10000)
x3 &lt;- 3*x1 + 4*x2 + rnorm(10000, sd = 0.0001)        # add a teeny bit of noise

X1 = matrix(c(x1, x2, x3), nrow = 10000) 

# Check for large values
solve(t(X1) %*% X1)</code></pre>
<pre><code>##           [,1]      [,2]       [,3]
## [1,]  88747.42 118329.88 -29582.469
## [2,] 118329.88 157773.15 -39443.288
## [3,] -29582.47 -39443.29   9860.822</code></pre>
</div>
<div id="penalized-regression-and-collinearity" class="section level2">
<h2>Penalized Regression and Collinearity</h2>
<p>Now that we understand the impact of collinearity, we can easily see how penalized regression addressess the issue. First, let’s first observe the solution the ridge regression problem (the results are the same for lasso regression):</p>
<p><span class="math display">\[\hat{\beta} = (\boldsymbol{X}^{T}\boldsymbol{X} + \lambda I)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\]</span></p>
<p>We are primarily concerned about finding that inverse - how do we know that the inverse exists, even when <span class="math inline">\(\boldsymbol{X}^{T}\boldsymbol{X}\)</span> is not? We’ll use same method used above to prove the invertibility - by checking if the matrix is positive definite. First, let <span class="math inline">\(\boldsymbol{a}\)</span> be any nonzero vector of length <span class="math inline">\(p\)</span>. Then, observe that:</p>
<p><span class="math display">\[
\begin{aligned}
  \boldsymbol{a}^{T}(\boldsymbol{X}^{T}\boldsymbol{X} + \lambda I)\boldsymbol{a}\\
  = \boldsymbol{a}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{a} + \boldsymbol{a}^{T}\lambda I\boldsymbol{a} \\
  = 0 + \lambda \boldsymbol{a}^{T}\boldsymbol{a} \\
  = \lambda \|\boldsymbol{a}\|_2^{2}
\end{aligned}
\]</span></p>
<p>Both terms are positive, so <span class="math inline">\(\lambda \|\boldsymbol{a}\|_2^{2} &gt; 0\)</span>, implying that the matrix is positive definite and hence invertible. This solves our first problem with the uniqueness of solutions. We now need to see how this affects the size of the values in the inverted matrix to verify if it decreases the variance.</p>
<p>I’ll begin by taking the Frobenius norm of the inverse when <span class="math inline">\(\lambda = 0\)</span> to get a sense of the magnitude of the values in the original matrix. Then, I’ll test a sequence of <span class="math inline">\(\lambda\)</span> values to see how the norm changes as the penalty is increased. This isn’t the most formal test, but it will at least make clear how the inverse matrix changes (and hence the variance of the estimates) in <span class="math inline">\(\lambda\)</span>.</p>
<pre class="r"><code>lambda &lt;-  seq(0, 100,  by = 1)

norms &lt;- vector(mode = &#39;numeric&#39;, length = 101)
for (i in seq_along(lambda)) {
  norms[i] &lt;- Matrix::norm(solve(t(X1) %*% X1 + lambda[i]*diag(3)))
}

data.frame(penalty = lambda, matrix_norm = norms) %&gt;%
  ggplot(., aes(penalty, log(matrix_norm))) + 
  geom_line() + 
  labs(title = &quot;Matrix norm as a function of Lambda&quot;) + 
  theme(plot.title = element_text(hjust = 0.5))</code></pre>
<p><img src="/willsmorgan.github.io/post/2018-09-03-solving-collinearity-with-penalized-regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Clearly, the regularization has a huge effect on the norm of the matrix - I even had to log the norm to make the graph more readable (otherwise it’d look like an L and wouldn’t be that interesting). This is the result we were looking for - the penalty helps keep down the variance of the estimates. This shouldn’t be terribly surprising, as that is the exact purpose of this technique. Ridge and lasso regression intentionally bias the coefficient estimates toward zero in order to shrink their variance for more generalizable results - it just so happens that it also solves the issue of collinearity.</p>
</div>
<div id="takeaways" class="section level2">
<h2>Takeaways</h2>
<p>Theoretically, multicollinearity can be a pretty nasty issue. It gives us an infinite amount of solutions for the parameter estimates, basically shutting down the entire problem. Practically speaking, perfect multicollinearity will almost never happen unless you have some sort of redundant coding scheme in your categorical variables (which is part of the reason why you have to leave one level out as your “base” level). When the issue does arise, it will most likely be a case of near-perfect multicollinearity. This also has some worrisome effects on our estimates, as their standard errors start to blow up, widening the confidence intervals. Penalized regression fortunately does not suffer from this because of the nature of the solutions. That is not to say however that penalized regression is meant to be used as a solution to the multicollinearity problem - it is intended more for walking the bias-variance tradeoff. If you do happen to end up with a dataset suffering from multicollinearity, you should be doing some serious inspection on the individual variables instead of jumping immediately to penalized regression.</p>
</div>
<div id="sources" class="section level2">
<h2>Sources:</h2>
<ul>
<li><p><a href="https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/lecture-17.pdf">Lecture Notes</a> from CMU’s Cosma Shalizi</p></li>
<li><p>Daniel Seita’s <a href="https://danieltakeshi.github.io/2016/08/05/a-useful-matrix-inverse-equality-for-ridge-regression/">blog post</a> on Ridge Regression inequalities</p></li>
<li><p><a href="http://www.math.harvard.edu/archive/20_spring_05/handouts/ch05_notes.pdf">More Lecture Notes</a> from a Harvard Math 20 section on eigenstuffs</p></li>
</ul>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/willsmorgan.github.io/tags/penalized-regression/">penalized regression</a>
  
</div>




    
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/willsmorgan.github.io/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

